\documentclass[../../main.tex]{subfiles}

\begin{document}
\chapter{Full Orthogonalization Method (FOM)}\label{ch:FOM}

\section{Overview and intuition}
FOM (Full Orthogonalization Method) is a Krylov-subspace method for solving the
linear system
\[ A\mathbf{x} = \mathbf{b}, \qquad A\in\mathbb{R}^{n\times n}, \]
where we build successive approximations
\(\mathbf{x}_m\in\mathbf{x}_0 + \mathcal{K}_m(A,\mathbf{r}_0)\) with the initial
residual \(\mathbf{r}_0=\mathbf{b}-A\mathbf{x}_0\). The characteristic
feature of FOM is the Galerkin condition with equal trial and test spaces
\(\mathcal{L}_m=\mathcal{K}_m\): the residual \(\mathbf{r}_m\) is orthogonal to
the entire Krylov subspace \(\mathcal{K}_m\). Intuitively, we insist that the
update to the approximate solution has removed all components of the residual
that lie in the directions in which we seek corrections.

While GMRES enforces a residual-minimization property (and can be viewed as a
Petrov--Galerkin method with test space \(A\mathcal{K}_m\)), FOM enforces the
Galerkin property directly on \(\mathcal{K}_m\). This difference has important
consequences for both theory and practice, which we discuss in later sections.

\section{Recap of the Arnoldi decomposition}
To work with Krylov subspaces numerically we use the Arnoldi algorithm to
generate an orthonormal basis \(V_m=[\mathbf{v}_1,\ldots,\mathbf{v}_m]\) of
\(\mathcal{K}_m(A,\mathbf{r}_0)\) and the corresponding upper Hessenberg matrix
\(\overline{H}_m\in\mathbb{R}^{(m+1)\times m}\) that satisfies
\begin{equation}\label{eq:arnoldi}
    A V_m = V_{m+1}\overline{H}_m = V_m H_m + h_{m+1,m}\,\mathbf{v}_{m+1}\mathbf{e}_m^\top,
\end{equation}
where \(H_m = \overline{H}_m(1:m,1:m) = V_m^\top A V_m\) is the projection of
\(A\) onto the subspace spanned by the columns of \(V_m\).

\section{Galerkin formulation and the small projected system}
Express the FOM approximation in the form
\[
    \mathbf{x}_m = \mathbf{x}_0 + V_m\mathbf{y}_m,
\]
and impose the Galerkin condition
\(V_m^\top\mathbf{r}_m=0\), where \(\mathbf{r}_m=\mathbf{b}-A\mathbf{x}_m\).
Using the Arnoldi relation \eqref{eq:arnoldi} we have
\[
    V_m^\top\mathbf{r}_m = V_m^\top\mathbf{r}_0 - V_m^\top A V_m\mathbf{y}_m
    = V_m^\top\mathbf{r}_0 - H_m\mathbf{y}_m = 0.
\]
Since \(V_m^\top\mathbf{r}_0=\beta\mathbf{e}_1\), \(\beta=\|\mathbf{r}_0\|_2\),
this leads to the small projected linear system
\begin{equation}\label{eq:proj_sys}
    H_m\mathbf{y}_m = \beta\mathbf{e}_1.
\end{equation}
The FOM iterate follows from \(\mathbf{x}_m=\mathbf{x}_0+V_m\mathbf{y}_m\).

\section{Residual formula and cost of evaluation}
The Arnoldi relation gives an inexpensive expression for the residual:
\begin{align*}
    \mathbf{r}_m & = \mathbf{r}_0 - A V_m\mathbf{y}_m = \beta\mathbf{v}_1 - V_{m+1}\overline{H}_m\mathbf{y}_m \\
                 & = -h_{m+1,m}\,\mathbf{v}_{m+1}\mathbf{e}_m^\top\mathbf{y}_m,
\end{align*}
because \(H_m\mathbf{y}_m=\beta\mathbf{e}_1\) by \eqref{eq:proj_sys}. Thus the
residual is a scalar multiple of the last Arnoldi vector \(\mathbf{v}_{m+1}\),
and
\begin{equation}\label{eq:res_norm}
    \|\mathbf{r}_m\|_2 = |h_{m+1,m}|\,|\mathbf{e}_m^\top\mathbf{y}_m|.
\end{equation}
We can therefore monitor convergence cheaply, by tracking the (scalar)
quantities \(h_{m+1,m}\) and the last component of \(\mathbf{y}_m\).

\section{FOM algorithm}
The practical method performs Arnoldi incrementally and solves the small system
\(H_j\mathbf{y}_j=\beta\mathbf{e}_1\) at each step to form the current
approximation.  The algorithm below summarizes the approach.
\begin{algorithm}[htbp]
    \begin{algorithmic}
        \Require $A \in \mathbb{R}^{n\times n},\, \mathbf{b}\in\mathbb{R}^n,\, \mathbf{x}_0,\, m_{\max},\, \text{tol}>0$
        \State $\mathbf{r}_0 = \mathbf{b}-A\mathbf{x}_0,\; \beta=\|\mathbf{r}_0\|_2,\; \mathbf{v}_1=\mathbf{r}_0/\beta$.
        \Ensure Approximations $\mathbf{x}_j$.
        \For{$j=1,2,\dots,m_{\max}$}
        \State Arnoldi step: compute $h_{1:j+1,j}$ and $\mathbf{v}_{j+1}$ (see Alg. \ref{alg:arnoldi-lecture}).
        \State Assemble $H_j=\overline{H}_j(1:j,1:j)$ and $V_j=[\mathbf{v}_1,\dots,\mathbf{v}_j]$.
        \State Solve $H_j\mathbf{y}_j=\beta\mathbf{e}_1$ and set $\mathbf{x}_j=\mathbf{x}_0+V_j\mathbf{y}_j$.
        \State Compute $\|\mathbf{r}_j\|_2 = |h_{j+1,j}|\,|\mathbf{e}_j^\top\mathbf{y}_j|$ using \eqref{eq:res_norm}.
        \If{$\|\mathbf{r}_j\|_2 \le \text{tol}$} \State Return $\mathbf{x}_j$. \EndIf
        \If{$h_{j+1,j}=0$} \State Break: a invariant subspace has been found and the exact solution is reached. \EndIf
        \EndFor
    \end{algorithmic}
    \caption{Full Orthogonalization Method (FOM)}
    \label{alg:fom}
\end{algorithm}

\section{Practical notes and comparisons}
\begin{itemize}
    \item \textbf{Per-iteration cost.} Each iteration requires one matrix-vector
          product $A\mathbf{v}_j$ and orthogonalization of that vector against the
          preceding \(j\) Arnoldi vectors.  The cost of orthogonalization grows with
          \(j\) (roughly $\mathcal{O}(j n)$) while the mat--vec depends on the structure
          (sparsity) of \(A\).
    \item \textbf{Solving the projected system.} At iteration \(j\) we must solve
          the dense but small linear system $H_j\mathbf{y}_j=\beta\mathbf{e}_1$. Direct
          solution is usually cheap for moderate \(j\) ($\mathcal{O}(j^3)$ per solve);
          however algorithms can reuse or update factorizations to reduce cost.
    \item \textbf{Breakdown and exactness.} If $h_{j+1,j}=0$ then the Arnoldi
          process has produced an invariant subspace of \(A\), and the exact solution is
          contained in \(\mathcal{K}_j\); FOM then finds the exact solution. In finite
          precision, loss of orthogonality is a concern and reorthogonalization may be
          necessary.
    \item \textbf{Relation to GMRES.} GMRES (Generalized Minimal Residual) uses the
          test space \(A\mathcal{K}_m\) which yields a residual-minimizing condition.
          In contrast, the Galerkin requirement of FOM is simpler to state and leads to
          a square projected system. For non-normal matrices GMRES often performs better
          in practice because it explicitly minimizes the residual norm.
    \item \textbf{Preconditioning.} Standard right-preconditioning preserves the
          Krylov structure and can be applied with FOM; left-preconditioning changes the
          residuals and must be used with care (or requires a different formulation).
\end{itemize}

\section{Summary}
FOM is an instructive Krylov method: it combines the Arnoldi decomposition with a
Galerkin projection on the Krylov space to reduce a large problem to a small
dense one.  The algorithm provides a cheap residual estimate and clear stopping
criteria, while trade-offs exist with other methods (notably GMRES) regarding
robustness, cost and memory use.

\end{document}