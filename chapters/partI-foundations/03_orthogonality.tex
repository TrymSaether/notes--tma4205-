\documentclass[../../main.tex]{subfiles}
\begin{document}

\chapter{Orthogonal Vectors and Subspaces}\label{chap:orthogonal-subspaces}

Orthogonality is one of the most important concepts in numerical linear algebra. It provides both theoretical insight and computational stability, making it essential for developing robust algorithms.

Let $V$ be an inner product space with inner product $\langle \cdot, \cdot \rangle$ and induced norm $\|\cdot\| = \sqrt{\langle \cdot, \cdot \rangle}$. The notion of orthogonality generalizes the familiar concept of perpendicularity from Euclidean geometry.

\begin{definition}{Orthogonal and Orthonormal Sets}{orthogonal-sets}
    A set of vectors $\{v_1, v_2, \ldots, v_n\} \subset V$ is \emph{orthogonal} if
    \[
        \langle v_i, v_j \rangle = 0 \quad \text{for all } i \neq j.
    \]

    The set is \emph{orthonormal} if it is orthogonal and each vector has unit norm:
    \[
        \langle v_i, v_j \rangle = \delta_{ij} \quad \text{for all } i, j \in \{1, 2, \ldots, n\},
    \]
    where $\delta_{ij}$ is the Kronecker delta.
\end{definition}

Orthonormal sets are particularly valuable because they form an ideal basis: coordinates can be computed easily using inner products, and the basis transformation matrix is orthogonal (or unitary), which preserves lengths and angles.

\section{Gram-Schmidt Process}\label{sec:gram-schmidt-process}
The Gram-Schmidt process (GS) goal is to construct an orthonormal basis from a given set of linearly independent vectors.
GS transforms any linearly independent set of vectors into an orthonormal set spanning the same subspace.

\begin{theorem}{Gram-Schmidt Theorem}{gram-schmidt-theorem}
    Let $\{v_1, v_2, \ldots, v_n\}$ be linearly independent vectors in an inner product space $V$. Then there exists a unique orthonormal set $\{q_1, q_2, \ldots, q_n\}$ such that
    \[
        \text{span}\{v_1, \ldots, v_k\} = \text{span}\{q_1, \ldots, q_k\} \quad \text{for } k = 1, 2, \ldots, n.
    \]
    Moreover, each $q_k$ can be written as a linear combination of $v_1, \ldots, v_k$.
\end{theorem}

The construction proceeds iteratively: at each step, we remove the components of the current vector that lie in the span of the previously computed orthonormal vectors, then normalize the result.

The orthonormal vectors are defined recursively:
\begin{align}
    q_1 & = \frac{v_1}{\|v_1\|},                                 \\
    q_k & = \frac{u_k}{\|u_k\|}, \quad k = 2, 3, \ldots, n,      \\
    u_k & = v_k - \sum_{j=1}^{k-1} \langle v_k, q_j \rangle q_j.
\end{align}

The key insight is that $u_k$ represents the component of $v_k$ orthogonal to the subspace spanned by $\{q_1, \ldots, q_{k-1}\}$.

\begin{algorithm}[H]
    \caption{Gram-Schmidt}
    \begin{algorithmic}
        \Require Linearly independent vectors $v_1, v_2, \ldots, v_n \in \mathbb{R}^m$
        \Ensure Orthonormal vectors $q_1, q_2, \ldots, q_n \in \mathbb{R}^m$
        \State $q_1 \leftarrow \frac{v_1}{\|v_1\|}$
        \For{$k = 2, \ldots, n$}
        \State $u_k \leftarrow v_k$
        \For{$j = 1, \ldots, k-1$}
        \State $u_k \leftarrow u_k - \langle v_k, q_j \rangle q_j$ \Comment{Project out $q_j$ component}
        \EndFor
        \State $q_k \leftarrow \frac{u_k}{\|u_k\|}$ \Comment{Normalize}
        \EndFor
    \end{algorithmic}
\end{algorithm}
\begin{remark}{Instability of Classical Gram-Schmidt}{classical-gram-schmidt}
    In the classical algorithm, rounding errors can cause significant loss of orthogonality, especially when the input vectors are nearly linearly dependent.
    The computed vectors may be far from orthogonal, undermining the algorithm's purpose.
\end{remark}

This motivates the modified Gram-Schmidt algorithm, which reorders the computations to improve stability.

\section{Modified Gram-Schmidt Process}\label{sec:modified-gram-schmidt-process}
The modified Gram-Schmidt algorithm (MGS) is significantly better numerical stability by performing orthogonalization sequentially against the already computed orthonormal vectors.
This reduces the accumulation of rounding errors and better preserves orthogonality in finite precision arithmetic.

\begin{algorithm}[H]
    \caption{Modified Gram-Schmidt}
    \begin{algorithmic}
        \Require Linearly independent vectors $v_1, v_2, \ldots, v_n \in \mathbb{R}^m$
        \Ensure Orthonormal vectors $q_1, q_2, \ldots, q_n \in \mathbb{R}^m$
        \For{$k = 1, \ldots, n$}
        \State $q_k \leftarrow v_k$
        \For{$j = 1, \ldots, k-1$}
        \State $r_{jk} \leftarrow \langle q_k, q_j \rangle$ \Comment{Compute projection coefficient}
        \State $q_k \leftarrow q_k - r_{jk} q_j$ \Comment{Remove $q_j$ component immediately}
        \EndFor
        \State $r_{kk} \leftarrow \|q_k\|$
        \State $q_k \leftarrow \frac{q_k}{r_{kk}}$ \Comment{Normalize}
        \EndFor
    \end{algorithmic}
\end{algorithm}

The key difference is that each orthogonalization step is performed immediately against the current (partially orthogonalized) vector, rather than against the original input vectors.

\paragraph{Comparison of CGS and MGS}

The key difference between CGS and MGS lies in \emph{when} the orthogonalized vectors are used:

\begin{center}
    \begin{tabular}{p{0.45\textwidth}|p{0.45\textwidth}}
        \textbf{Classical Gram-Schmidt (CGS)}                                                   & \textbf{Modified Gram-Schmidt (MGS)}                              \\ \hline
        Project $v_k$ against \emph{all} $q_1, \ldots, q_{k-1}$ using the \emph{original} $v_k$ & Project $v_k$ \emph{sequentially}, updating after each projection \\[0.5em]
        \begin{algorithmic}
            \State $u_k \leftarrow v_k$
            \For{$j = 1, \ldots, k-1$}
            \State $r_{jk} \leftarrow \langle \textcolor{red}{v_k}, q_j \rangle$
            \State $u_k \leftarrow u_k - r_{jk} q_j$
            \EndFor
            \State $q_k \leftarrow u_k / \|u_k\|$
        \end{algorithmic}
                                                                                                &
        \begin{algorithmic}
            \State $q_k \leftarrow v_k$
            \For{$j = 1, \ldots, k-1$}
            \State $r_{jk} \leftarrow \langle \textcolor{blue}{q_k}, q_j \rangle$
            \State $q_k \leftarrow q_k - r_{jk} q_j$
            \EndFor
            \State $q_k \leftarrow q_k / \|q_k\|$
        \end{algorithmic}
        \\[0.5em] \hline
        Uses original vector $v_k$ for all inner products                                       & Uses progressively orthogonalized $q_k$ for inner products        \\
        Accumulates rounding errors                                                             & Reduces error propagation                                         \\
        Can lose orthogonality for ill-conditioned matrices                                     & Better preserves orthogonality                                    \\
    \end{tabular}
\end{center}

\begin{example}{Numerical example}{gs_vs_mgs_comparison}
    For $A = \begin{bsmallmatrix} 1 & 1 \\ 1 & 0 \\ 0 & 1 \end{bsmallmatrix}$, both methods yield (in exact arithmetic):
    \[
        \widetilde{Q} = \begin{bmatrix}
            \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}  \\
            \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{6}} \\
            0                  & \frac{2}{\sqrt{6}}
        \end{bmatrix}, \qquad
        R = \begin{bmatrix}
            \sqrt{2} & \frac{1}{\sqrt{2}} \\
            0        & \frac{\sqrt{6}}{2}
        \end{bmatrix}.
    \]

    However, in finite precision:
    \begin{itemize}
        \item \textbf{CGS}: For nearly dependent columns, $\|Q^\top Q - I\|$ can be $O(\kappa(A)\varepsilon)$ where $\kappa(A)$ is the condition number
        \item \textbf{MGS}: Achieves $\|Q^\top Q - I\| = O(\varepsilon)$, maintaining orthogonality to machine precision
    \end{itemize}
\end{example}

While CGS provides geometric intuition, MGS is strongly preferred for numerical computation. For even better stability, use Householder QR (Section~\ref{sec:householder-reflections}).


\section{QR Decomposition}\label{sec:qr-decomposition}
The QR decomposition is a fundamental matrix factorization that expresses any matrix $A \in \mathbb{C}^{m \times n}$ (with $m \geq n$) as
\[
    A = QR,
\]
where $Q\in\mathbb{C}^{m\times m}$ is unitary and $R\in\mathbb{C}^{m\times n}$ is upper triangular (or upper trapezoidal when $m>n$).

When $A$ has full column rank, one commonly uses the \emph{thin} (or \emph{economy}) QR factorization:
\[
    A = \widetilde{Q} R,\qquad \widetilde{Q}\in\mathbb{C}^{m\times n},\;\widetilde{Q}^H \widetilde{Q}=I_n,\;R\in\mathbb{C}^{n\times n}\ \text{upper triangular},
\]
which is unique up to multiplication of $\widetilde{Q}$ on the right by a diagonal unitary matrix (in the real case, by signs).

The QR decomposition has numerous applications:
\begin{itemize}
    \item Solving least squares problems: $\min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2$
    \item Computing matrix eigenvalues (QR algorithm)
    \item Orthogonalizing vectors (Gram-Schmidt process)
    \item Numerical solution of linear systems
\end{itemize}

There are several algorithms for computing the QR decomposition. The Gram-Schmidt process provides geometric insight and is covered in this chapter (\autoref{chap:orthogonal-subspaces}). For practical computation, Householder reflections (discussed below) are the most numerically stable and widely used method.


\section{Householder Reflections}\label{sec:householder-reflections}
Householder reflections are orthogonal transformations that reflect vectors across hyperplanes.
They are widely used in numerical linear algebra for QR decomposition and other orthogonalizations due to their numerical stability and efficiency.

\begin{definition}{Householder reflection}{def:householder}
    Let $\mathbf{u} \in \mathbb{R}^n$ be a unit vector. The Householder matrix is
    \[
        H \;=\; I - 2uu^\top,
    \]
    which represents the reflection across the hyperplane orthogonal to $\mathbf{u}$.
\end{definition}

Geometrically, $H$ sends $\mathbf{u} \mapsto -\mathbf{u}$ and fixes every vector orthogonal to $\mathbf{u}$.

\begin{figure}[htb]
    \centering
    \input{figures/CH_01/householder.tikz}
    \caption{Householder reflection of $\mathbf{x}$ across the hyperplane orthogonal to $\mathbf{u}$. The projection $\pi_u(\mathbf{x})$ is in grey and the reflected vector $Hx$ is in blue.}
    \label{fig:householder-reflection}
\end{figure}

\begin{proposition}{Basic properties}{prop:householder-props}
    Let $H=I-2uu^\top$ with $\|\mathbf{u}\|=1$. Then
    \begin{enumerate}
        \item $H^\top=H$ (symmetric),
        \item $H^\top H=I$ (orthogonal),
        \item $H^{-1}=H$ (involutory),
        \item $H\mathbf{u} = -\mathbf{u}$ and $H\mathbf{v} = \mathbf{v}$ for all $\mathbf{v} \perp \mathbf{u}$,
        \item $\det(H)=-1$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item is immediate.
        \item Expand $(I-2uu^\top)^2=I-4uu^\top+4u(\mathbf{u}^\top \mathbf{u})\mathbf{u}^\top=I$.
        \item Follows from 1. and 2.
        \item $H\mathbf{u}=\mathbf{u}-2\mathbf{u}(\mathbf{u}^\top \mathbf{u})=\mathbf{u}-2\mathbf{u}=-\mathbf{u}$.
              If $\mathbf{v}\perp \mathbf{u}$, then $H\mathbf{v}=\mathbf{v}-2\mathbf{u}(\mathbf{u}^\top \mathbf{v})=\mathbf{v}$.
        \item $\det(H)=\det(I-2uu^\top)=\det(I)\det(I-2u^\top u)=1\cdot(1-2)=-1$.
    \end{enumerate}
\end{proof}

\section{Vector Annihilation with Householder Reflections}\label{sec:householder-vector-annihilation}

A key application of Householder reflections is to transform a given vector into a multiple of a standard basis vector, effectively zeroing out all but one component.

\begin{theorem}{Vector annihilation via Householder reflection}{thm:householder-vector-annihilation}
    For any nonzero $\mathbf{x}\in\mathbb{R}^n$, there exists a Householder matrix $H=I-2\mathbf{u}\mathbf{u}^\top$ such that
    \[
        H\mathbf{x} = \sigma \mathbf{e}_1, \quad \text{where } \sigma = \pm \|\mathbf{x}\|_2.
    \]
    A numerically stable choice is $\sigma = -\operatorname{sign}(x_1)\,\|\mathbf{x}\|_2$ (with $\operatorname{sign}(0)=1$).
    The unit vector $\mathbf{u}$ is constructed as
    \[
        \mathbf{v}=\mathbf{x}-\sigma \mathbf{e}_1, \qquad \mathbf{u}=\frac{\mathbf{v}}{\|\mathbf{v}\|_2}.
    \]
\end{theorem}

\begin{proof}
    Let $\mathbf{v}=\mathbf{x}-\sigma \mathbf{e}_1$ and $\mathbf{u}=\mathbf{v}/\|\mathbf{v}\|_2$ with $\sigma=\pm\|\mathbf{x}\|_2$. Then
    \[
        H\mathbf{x}=\mathbf{x}-2\mathbf{u}(\mathbf{u}^\top \mathbf{x}) = \mathbf{x} - \frac{2\mathbf{v}(\mathbf{v}^\top \mathbf{x})}{\|\mathbf{v}\|_2^2}.
    \]
    Computing the inner product:
    \[
        \mathbf{v}^\top \mathbf{x} = (\mathbf{x}-\sigma \mathbf{e}_1)^\top \mathbf{x} = \|\mathbf{x}\|_2^2 - \sigma x_1.
    \]
    Computing the squared norm:
    \[
        \|\mathbf{v}\|_2^2 = (\mathbf{x}-\sigma \mathbf{e}_1)^\top(\mathbf{x}-\sigma \mathbf{e}_1)=\|\mathbf{x}\|_2^2 - 2\sigma x_1 + \sigma^2 = 2(\|\mathbf{x}\|_2^2-\sigma x_1),
    \]
    where the last equality uses $\sigma^2=\|\mathbf{x}\|_2^2$. Therefore
    \[
        \frac{2\mathbf{v}(\mathbf{v}^\top \mathbf{x})}{\|\mathbf{v}\|_2^2}=\frac{2\mathbf{v}(\|\mathbf{x}\|_2^2-\sigma x_1)}{2(\|\mathbf{x}\|_2^2-\sigma x_1)}=\mathbf{v},
    \]
    and thus $H\mathbf{x}=\mathbf{x}-\mathbf{v}=\sigma \mathbf{e}_1$.

    The sign choice $\sigma=-\operatorname{sign}(x_1)\|\mathbf{x}\|_2$ maximizes $|\mathbf{v}|$ and avoids catastrophic cancellation when computing $\mathbf{v}=\mathbf{x}-\sigma \mathbf{e}_1$. If $x_1>0$, then $\sigma<0$, making $\mathbf{v}=\mathbf{x}+|\sigma|\mathbf{e}_1$ an addition rather than a subtraction.
\end{proof}

\begin{algorithm}[H]
    \caption{Construct Householder Vector for Annihilation}
    \begin{algorithmic}
        \Require Nonzero vector $\mathbf{x}\in\mathbb{R}^n$
        \Ensure Unit vector $\mathbf{u}$ such that $(I-2\mathbf{u}\mathbf{u}^\top)\mathbf{x}=\sigma \mathbf{e}_1$, where $\sigma=-\operatorname{sign}(x_1)\|\mathbf{x}\|_2$
        \State $\alpha \gets \|\mathbf{x}\|_2$
        \State $\sigma \gets -\operatorname{sign}(x_1)\,\alpha$ \Comment{$\operatorname{sign}(0)=1$ by convention}
        \State $\mathbf{v} \gets \mathbf{x}-\sigma \mathbf{e}_1$
        \State $\mathbf{u} \gets \mathbf{v}/\|\mathbf{v}\|_2$
        \State \Return $\mathbf{u}$
    \end{algorithmic}
\end{algorithm}

\section{QR decomposition via Householder reflections}

\begin{theorem}{Householder QR}{thm:householder-qr}
    Let $A\in\mathbb{R}^{m\times n}$ with $m\ge n$. There exist Householder matrices $H_1,\dots,H_n$ such that
    \[
        R \;=\; H_n\cdots H_2 H_1 A
    \]
    is upper triangular, and with $Q=(H_n\cdots H_1)^\top$ we have $A=QR$.
\end{theorem}

Algorithmically, process columns $k=1,\dots,n$: apply a Householder reflection to zero out entries $k+1{:}m$ in column $k$, leaving previously formed zeros undisturbed.

\begin{algorithm}[H]
    \caption{QR Decomposition via Householder Reflections (full \(Q\))}
    \begin{algorithmic}
        \Require $A \in \mathbb{R}^{m \times n}$ with $m \ge n$
        \Ensure $Q \in \mathbb{R}^{m \times m}$ (orthogonal), $R \in \mathbb{R}^{m \times n}$ (upper-trapezoidal) s.t. $A = QR$
        \State $Q \gets I_m$
        \For{$k = 1,2,\ldots,n$}
        \State $\mathbf{x} \gets A_{k:m,\,k}$
        \If{$\mathbf{x} \ne \mathbf{0}$}
        \State $\alpha \gets \|\mathbf{x}\|_2$
        \State $\sigma \gets -\operatorname{sign}(x_1)\,\alpha$ \Comment{$\operatorname{sign}(0)=1$}
        \State $\mathbf{v} \gets \mathbf{x} - \sigma\,\mathbf{e}_1$
        \State $\beta \gets \|\mathbf{v}\|_2$
        \If{$\beta > 0$}
        \State $\mathbf{u} \gets \mathbf{v}/\beta$
        \State $H_k \gets I_{m-k+1} - 2\,\mathbf{u}\mathbf{u}^\top$ \Comment{Reflector}
        \State $A_{k:m,\,k:n} \gets H_k\,A_{k:m,\,k:n}$
        \State $Q_{k:m,\,:} \gets H_k\,Q_{k:m,\,:}$
        \EndIf
        \EndIf
        \EndFor
        \State $R \gets A$ \Comment{$A \implies$ upper-trapezoidal}
        \State $Q \gets Q^\top$ \Comment{$Q\gets H_n\cdots H_1 \implies Q=H_1\cdots H_n$}
    \end{algorithmic}
\end{algorithm}

\begin{remark}{Stability and cost}{rem:householder-advantages}
    Householder QR is backward stable; the computed $\widehat{Q}$ is orthogonal to machine precision, and $\widehat{R}$ is the exact $R$ of a nearby $A+\Delta A$ with small relative $\|\Delta A\|$. The flop count is
    \[
        2mn^2 - \tfrac{2}{3}n^3\quad (m\ge n),
    \]
    and blocked implementations use cache-efficient matrixâ€“matrix updates.
\end{remark}

\begin{example}{A $3\times 2$ example}{householder-qr}
    Let $A=\begin{bmatrix}1&1\\1&0\\0&1\end{bmatrix}$.

    \emph{Step 1.}
    \begin{align*}
        a_1 & =(1,1,0)^\top, \quad \|a_1\|=\sqrt2,\quad \sigma_1=-\sqrt2 \\
        v_1 & =a_1-\sigma_1 e_1=(1+\sqrt2,\,1,\,0)^\top                  \\
        u_1 & =\frac{v_1}{\|v_1\|}
    \end{align*}
    Then
    \[
        H_1=I-2u_1u_1^\top
        =\begin{bmatrix}
            -\tfrac{1}{\sqrt2} & -\tfrac{1}{\sqrt2} & 0 \\
            -\tfrac{1}{\sqrt2} & \tfrac{1}{\sqrt2}  & 0 \\
            0                  & 0                  & 1
        \end{bmatrix},
        \qquad
        A^{(1)}=H_1A=
        \begin{bmatrix}
            -\sqrt2 & -\tfrac{1}{\sqrt2} \\
            0       & -\tfrac{1}{\sqrt2} \\
            0       & 1
        \end{bmatrix}.
    \]
    \emph{Step 2.} Take the subvector $\mathbf{x}=\bigl(-\tfrac{1}{\sqrt2},\,1\bigr)^\top$, $\|\mathbf{x}\|=\sqrt{\tfrac32}=\tfrac{\sqrt6}{2}$, $\sigma_2=\tfrac{\sqrt6}{2}$.
    Build a $2\times2$ reflector and embed:
    \[
        H_2=
        \begin{bmatrix}
            1 & 0                      & 0                      \\
            0 & -\tfrac{1}{\sqrt3}     & \tfrac{\sqrt2}{\sqrt3} \\
            0 & \tfrac{\sqrt2}{\sqrt3} & \tfrac{1}{\sqrt3}
        \end{bmatrix}.
    \]
    Then
    \[
        R=H_2A^{(1)}=
        \begin{bmatrix}
            -\sqrt2 & -\tfrac{1}{\sqrt2} \\
            0       & \tfrac{\sqrt6}{2}  \\
            0       & 0
        \end{bmatrix},
        \qquad
        Q=H_1H_2.
    \]
    The thin factor is
    \[
        \widetilde{Q}=\begin{bmatrix}
            -\tfrac{1}{\sqrt2} & \tfrac{1}{\sqrt6}  \\
            -\tfrac{1}{\sqrt2} & -\tfrac{1}{\sqrt6} \\
            0                  & \tfrac{2}{\sqrt6}
        \end{bmatrix},
        \quad
        \text{so }A=\widetilde{Q}R.
    \]
\end{example}

\end{document}
