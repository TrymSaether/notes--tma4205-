\chapter{Introduction}

This course covers algorithms for solving linear systems $Ax = b$, eigenvalue problems, and matrix factorizations on computers with finite precision arithmetic.

We study three main categories of \emph{problems}:
\begin{itemize}
    \item Direct methods: Gaussian elimination, LU/QR/Cholesky factorizations
    \item Iterative methods: Krylov subspace methods, multigrid, domain decomposition
    \item Eigenvalue computation: Power method, QR algorithm, Arnoldi/Lanczos methods
\end{itemize}

The key challenge is balancing computational cost with numerical accuracy. Round-off errors accumulate differently across algorithms, and problem conditioning determines which methods remain stable.
We emphasize implementation details and complexity analysis. Most real problems involve sparse matrices where structure must be exploitedâ€”dense matrix algorithms often fail due to memory and time constraints.

The material follows Saad (2003) with focus on methods used in practice for large-scale scientific computing.

\section{Large Sparse Problems}
We focus on matrices that are large ($n\gtrsim 10^4$) and sparse. Let $N_z(A)$ denote the number of nonzeros. Storage and matrix--vector products scale with $O(N_z(A))$, whereas dense factorizations cost $O(n^3)$ flops and $O(n^2)$ memory. Consequently, iterative methods driven by sparse matrix--vector products are central to modern scientific computing.
