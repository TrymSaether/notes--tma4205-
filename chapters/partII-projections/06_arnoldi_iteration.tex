\chapter{Arnoldi Iteration}
\label{chap:arnoldi}

The Arnoldi iteration is a fundamental algorithm that constructs an orthonormal basis for Krylov subspaces while simultaneously revealing the structure of how a matrix $A$ acts on these subspaces. This elegant process forms the mathematical foundation for many of the most successful iterative solvers, including GMRES and FOM.

\section{Motivation}

Consider the sequence of vectors generated by repeatedly applying a matrix $A$ to an initial vector $\mathbf{r}_0$:
\[
\mathbf{r}_0, \quad A\mathbf{r}_0, \quad A^2\mathbf{r}_0, \quad A^3\mathbf{r}_0, \quad \ldots
\]

The \emph{Krylov subspace} of dimension $m+1$ is the span of the first $m+1$ such vectors:
\[
\mathcal{K}_{m+1}(A,\mathbf{r}_0) := \text{span}\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \ldots, A^m\mathbf{r}_0\}
\]

While this sequence captures how $A$ acts on $\mathbf{r}_0$, these vectors typically become increasingly aligned and numerically dependent. The Arnoldi process transforms this unwieldy sequence into an orthonormal basis that preserves all the essential information about $A$'s action on the Krylov subspace.

\begin{learninggoals}[title={Learning goals}]
  After this section you can
  \begin{itemize}[nosep]
    \item understand why orthogonal Krylov bases are crucial for numerical stability,
    \item construct an orthonormal basis of a Krylov subspace using the Arnoldi algorithm,
    \item derive and apply the fundamental Arnoldi relation $AV_m = V_{m+1}\overline{H}_m$,
    \item explain how this relation enables efficient projection methods like FOM and GMRES,
    \item recognize breakdown conditions and understand reorthogonalization strategies.
  \end{itemize}
\end{learninggoals}

\section{The Arnoldi Algorithm}

The key insight of Arnoldi iteration is to apply the Gram-Schmidt process \emph{incrementally} as we build up the Krylov subspace. Starting with $\mathbf{v}_1 = \mathbf{r}_0/\|\mathbf{r}_0\|_2$, we:

\begin{enumerate}
\item Generate the next Krylov vector: $A\mathbf{v}_j$
\item Orthogonalize it against all previous vectors: $\mathbf{v}_1, \ldots, \mathbf{v}_j$
\item Normalize to get the next basis vector: $\mathbf{v}_{j+1}$
\item Record the coefficients from orthogonalization in matrix $\overline{H}_m$
\end{enumerate}

\begin{algorithm}[H]
  \caption{Arnoldi Iteration (Modified Gram-Schmidt)}\label{alg:arnoldi}
  \begin{algorithmic}[1]
    \Require $A\in\mathbb{R}^{n\times n}$, $\mathbf{b}\in\mathbb{R}^n$, $\mathbf{x}_0$ (init. guess), $m$ (num. steps)
    \Ensure  $V_{m+1} = [\mathbf{v}_1, \ldots, \mathbf{v}_{m+1}]$ (Orth. basis of $\mathcal{K}_{m+1}$, $n\times m$), $\overline{H}_m$ (Upper Hessenberg, $(m+1)\times m$)
    \State $\mathbf{r}_0 \gets \mathbf{b} - A\mathbf{x}_0$ \Comment{Init. residual}
    \State $\beta \gets \|\mathbf{r}_0\|_2$
    \State $\mathbf{v}_1 \gets \mathbf{r}_0/\beta$
    \For{$j = 1, 2, \ldots, m$}
      \State $\mathbf{w} \gets A\mathbf{v}_j$
      \For{$i = 1, 2, \ldots, j$}
        \State $h_{i,j} \gets \langle\mathbf{v}_i, \mathbf{w}\rangle$
        \State $\mathbf{w} \gets \mathbf{w} - h_{i,j}\mathbf{v}_i$
      \EndFor
      \State $h_{j+1,j} \gets \|\mathbf{w}\|_2$
      \If{$h_{j+1,j} = 0$}
        \State \textbf{break} \Comment{Happy breakdown (Krylov subspace is $A$-invariant)}
      \EndIf
      \State $\mathbf{v}_{j+1} \gets \mathbf{w}/h_{j+1,j}$
    \EndFor
  \end{algorithmic}
\end{algorithm}

\section{Arnoldi Relation}

After $m$ steps, we have constructed:
\begin{align}
V_{m+1} &= [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_{m+1}] \in \mathbb{R}^{n \times (m+1)} \quad \text{(orthonormal basis)} \\
V_m &= [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m] \in \mathbb{R}^{n \times m} \quad \text{(first $m$ columns)} \\
\overline{H}_m &= \begin{bmatrix}
h_{1,1} & h_{1,2} & h_{1,3} & \cdots & h_{1,m} \\
h_{2,1} & h_{2,2} & h_{2,3} & \cdots & h_{2,m} \\
0 & h_{3,2} & h_{3,3} & \cdots & h_{3,m} \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & \cdots & 0 & h_{m+1,m-1} & h_{m+1,m} \\
0 & \cdots & 0 & 0 & h_{m+1,m}
\end{bmatrix} \in \mathbb{R}^{(m+1) \times m}
\end{align}

The matrix $\overline{H}_m$ is \emph{upper Hessenberg} (zero below the first subdiagonal) and encodes all the orthogonalization coefficients from the Gram-Schmidt process.

\paragraph{The Arnoldi Relation.}
The fundamental relationship is:
\begin{equation}\label{eq:arnoldi-relation}
\boxed{AV_m = V_{m+1}\overline{H}_m}
\end{equation}

This compact relation captures a profound fact: \emph{the action of the large matrix $A$ on the Krylov subspace is completely characterized by the small Hessenberg matrix $\overline{H}_m$}.

\paragraph{Understanding the relation.} Column by column, this says:
\[
A\mathbf{v}_j = h_{1,j}\mathbf{v}_1 + h_{2,j}\mathbf{v}_2 + \cdots + h_{j,j}\mathbf{v}_j + h_{j+1,j}\mathbf{v}_{j+1}
\]
In other words, $A\mathbf{v}_j$ lies in $\text{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_{j+1}\}$, which is exactly what we'd expect since $A\mathbf{v}_j$ is the $(j+1)$-th Krylov vector (before orthogonalization).

\paragraph{Alternative form.}
We can also write:
\[
AV_m = V_m H_m + h_{m+1,m}\mathbf{v}_{m+1}\mathbf{e}_m^T
\]
where $H_m = \overline{H}_m(1:m, 1:m)$ is the $m \times m$ upper part, and $\mathbf{e}_m$ is the $m$-th standard basis vector. The term $h_{m+1,m}\mathbf{v}_{m+1}\mathbf{e}_m^T$ represents how $A$ "escapes" the $m$-dimensional subspace.

\section{Geometric Interpretation}

The Arnoldi process can be viewed as:
\begin{itemize}
\item \textbf{Projection perspective}: We're finding the best representation of $A$ when restricted to the Krylov subspace. The matrix $H_m = V_m^T AV_m$ is the projection of $A$ onto $\mathcal{K}_m(A,\mathbf{r}_0)$.
\item \textbf{Similarity perspective}: The relation $AV_m = V_m H_m + \text{residual}$ shows that $A$ and $H_m$ are nearly similar, with the "residual" measuring how much information we lose by working in the smaller subspace.
\end{itemize}

\section{Breakdown Conditions}

\paragraph{Happy breakdown.} If $h_{j+1,j} = 0$ for some $j < m$, then $\mathbf{w} = A\mathbf{v}_j$ lies entirely in $\text{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_j\}$. This means:
\[
A(\mathcal{K}_j(A,\mathbf{r}_0)) \subseteq \mathcal{K}_j(A,\mathbf{r}_0)
\]
The Krylov subspace is \emph{$A$-invariant}, and we have found an exact invariant subspace. This is called "happy breakdown" because:
\begin{itemize}
\item We can solve linear systems exactly within this subspace
\item We have found exact eigenvalue/eigenvector information
\item No further iteration is needed
\end{itemize}

\paragraph{Near-breakdown.} In finite precision arithmetic, $h_{j+1,j}$ may be very small but nonzero, leading to numerical instability. This requires careful handling through techniques like deflation or restarting.

\section{Numerical Stability and Reorthogonalization}

The Modified Gram-Schmidt process used in Algorithm~\ref{alg:arnoldi} is more numerically stable than Classical Gram-Schmidt, but orthogonality can still be lost due to:
\begin{itemize}
\item Round-off errors accumulating over many iterations
\item Nearly linearly dependent Krylov vectors
\item Ill-conditioned matrices $A$
\end{itemize}

\paragraph{Reorthogonalization strategies:}
\begin{itemize}
\item \textbf{Full reorthogonalization}: Orthogonalize twice against all previous vectors
\item \textbf{Selective reorthogonalization}: Monitor loss of orthogonality and reorthogonalize only when needed
\item \textbf{Periodic reorthogonalization}: Reorthogonalize every few steps
\end{itemize}

\section{Computational Complexity}

\paragraph{Per iteration cost:}
\begin{itemize}
\item One matrix-vector product: $A\mathbf{v}_j$ costs $O(\text{nnz}(A))$ or $O(n^2)$ flops
\item Orthogonalization: $j$ inner products and $j$ vector updates cost $O(jn)$ flops
\end{itemize}

\paragraph{Total cost for $m$ iterations:}
\[
\text{Flops} = O\left(m \cdot \text{cost}(A\mathbf{v}) + \sum_{j=1}^m jn\right) = O(m \cdot \text{cost}(A\mathbf{v}) + m^2n)
\]

\paragraph{Storage requirements:}
\begin{itemize}
\item Basis vectors $V_{m+1}$: $O(nm)$ memory
\item Hessenberg matrix $\overline{H}_m$: $O(m^2)$ memory
\item \textbf{Total}: $O(nm + m^2)$ memory
\end{itemize}

The storage requirement $O(nm)$ can become prohibitive for large $m$, motivating restarted variants.

\section{Applications: The Foundation for Krylov Solvers}

The Arnoldi relation \eqref{eq:arnoldi-relation} enables two major classes of iterative solvers:

\paragraph{Full Orthogonalization Method (FOM):}
Uses Galerkin projection: find $\mathbf{x}_m \in \mathbf{x}_0 + \mathcal{K}_m(A,\mathbf{r}_0)$ such that the residual is orthogonal to the Krylov subspace.

\paragraph{Generalized Minimal Residual (GMRES):}
Uses minimal residual projection: find $\mathbf{x}_m \in \mathbf{x}_0 + \mathcal{K}_m(A,\mathbf{r}_0)$ that minimizes $\|A\mathbf{x}_m - \mathbf{b}\|_2$.

Both methods reduce the original $n \times n$ problem to an $m \times m$ problem involving $H_m$ or $\overline{H}_m$.

\begin{summary}{The Power of Arnoldi}{arnoldi-summary}
  The Arnoldi iteration transforms the numerically unstable sequence $\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \ldots\}$ into:
  \begin{itemize}
    \item A numerically stable orthonormal basis $V_{m+1}$ of $\mathcal{K}_{m+1}(A,\mathbf{r}_0)$
    \item A compact representation $\overline{H}_m$ of how $A$ acts on the Krylov subspace
    \item The fundamental relation $AV_m = V_{m+1}\overline{H}_m$ that enables efficient projections
    \item A foundation for optimal Krylov subspace methods like GMRES
    \item Natural stopping criteria through happy breakdown detection
  \end{itemize}
  This combination of numerical stability, dimensional reduction, and preserved spectral information makes Arnoldi iteration one of the most important algorithms in numerical linear algebra.
\end{summary}

The specific applications of this framework to solve linear systems and eigenvalue problems are developed in Chapter~\ref{chap:projection-methods}, where we'll see how the Arnoldi relation enables both exact solutions (via FOM) and optimal approximations (via GMRES).
