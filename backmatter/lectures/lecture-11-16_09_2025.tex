\section{Lecture 11: 16.09.2025}

Go from Arnoldi $\to$ \emph{Lanczos} (symmetric case) $\to$ \emph{conjugate gradient} (CG).

We first start with the assumption that $A$ is \emph{symmetric and positive definite} (SPD), i.e., $A = A^T \succ 0$.

\subsection{Recap: Arnoldi iteration}

\begin{algorithm}[H]
    \caption{Arnoldi iteration where $A$ is SPD}
    \begin{algorithmic}[0]
        \Require $A, \mathbf{b}, \mathbf{x}_0, m$
        \State $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0$
        \State $\beta = \|\mathbf{r}_0\|_2$
        \State $\mathbf{v}_1 = \frac{\mathbf{r}_0}{\beta}$
        \For{$j = 1, 2, \ldots, m$}
        \State $h_{ij} = \langle A\mathbf{v}_j, \mathbf{v}_i \rangle$ for $i = 1, 2, \ldots, j$
        \State $\mathbf{w}_j = A\mathbf{v}_j - \sum_{i=1}^j h_{ij}\mathbf{v}_i$
        \State $h_{j+1,j} = \|\mathbf{w}_j\|_2$
        \If{$h_{j+1,j} = 0$}
        \State Stop
        \EndIf
        \State $\mathbf{v}_{j+1} = \frac{\mathbf{w}_j}{h_{j+1,j}}$
        \EndFor

        \Return $V_m = [\mathbf{v}_1, \ldots, \mathbf{v}_m]$, $\bar{H}_m =
            \begin{bmatrix}
                H_m \\
                h_{m+1,m}\mathbf{e}_m^T
            \end{bmatrix}$
    \end{algorithmic}
\end{algorithm}

Then we have the Arnoldi relation
\begin{align*}
    AV_m = V_{m+1}\bar{H}_m \\
    V_m^\top AV_m = H_m
\end{align*}
Where we solve the reduced linear system:
\begin{align*}
    \mathbf{x}_m & = \mathbf{x}_0 + V_m H_m^{-1} V_m^\top \mathbf{r}_0                                \\
                 & = \mathbf{x}_0 + V_m H_m^{-1} \beta \mathbf{e}_1, \quad \beta = \|\mathbf{r}_0\|_2 \\
    \mathbf{x}_m & = \mathbf{x}_0 + V_m \mathbf{y}_m
\end{align*}

How can this be simplified if $A = A^\top$?

In this case $H_m = V_m^\top A V_m = H_m^\top$ is symmetric, and since it is upper Hessenberg it must be tridiagonal.
$H_m$ is then tridiagonal and symmetric, i.e., $H_m$ has the form:
\[
    H_m =
    \begin{bmatrix}
        \alpha_1 & \beta_2  & 0       & \cdots  & 0        \\
        \beta_2  & \alpha_2 & \beta_3 & \cdots  & 0        \\
        0        & \beta_3  & \ddots  & \ddots  & \vdots   \\
        \vdots   & \vdots   & \ddots  & \ddots  & \beta_m  \\
        0        & 0        & 0       & \beta_m & \alpha_m
    \end{bmatrix}
\]

\subsection{Lanczos iteration}
\begin{algorithm}[H]
    \caption{Lanczos: Arnoldi for symmetric $A=A^\top$}
    \begin{algorithmic}[0]
        \Require $A, \mathbf{b}, \mathbf{x}_0, m$
        \State $\beta_1 = 0$
        \State $\mathbf{v}_0 = 0$
        \State $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0$
        \State $\beta = \|\mathbf{r}_0\|_2$
        \State $\mathbf{v}_1 = \dfrac{\mathbf{r}_0}{\beta}$
        \For{$j = 1, 2, \ldots, m$}
        \State $\mathbf{w}_j = A\mathbf{v}_j - \beta_j \mathbf{v}_{j-1}$, where $\beta_1 \mathbf{v}_0 = 0$
        \State $\alpha_j = \langle \mathbf{w}_j, \mathbf{v}_j \rangle$
        \State $\mathbf{w}_j = \mathbf{w}_j - \alpha_j \mathbf{v}_j$
        \State $\beta_{j+1} = \|\mathbf{w}_j\|_2$
        \If{$\beta_{j+1} = 0$} \textbf{Stop}
        \EndIf
        \State $\mathbf{v}_{j+1} = \frac{\mathbf{w}_j}{\beta_{j+1}}$
        \EndFor
        \Return $V_{m+1} = [\mathbf{v}_1, \ldots, \mathbf{v}_{m+1}]$
        \State $T_m = \operatorname{tridiag}(\beta_i, \alpha_i, \beta_{i+1}), \quad i=1,\ldots,m$
        \State $\mathbf{x}_m = \mathbf{x}_0 + V_m T_m^{-1} \beta \mathbf{e}_1$
        \State \textbf{Solve:} $T_m \mathbf{y}_m = \beta \mathbf{e}_1$
    \end{algorithmic}
\end{algorithm}

We solve the tridiagonal system:
\[
    T_m \mathbf{y}_m = \beta \mathbf{e}_1
\]
using \emph{LU} factorization:
\begin{align*}
    T_m                                                   & = L_m U_m \\
    \begin{bmatrix}
        \alpha_1 & \beta_2  & 0       & \cdots  & 0        \\
        \beta_2  & \alpha_2 & \beta_3 & \cdots  & 0        \\
        0        & \beta_3  & \ddots  & \ddots  & \vdots   \\
        \vdots   & \vdots   & \ddots  & \ddots  & \beta_m  \\
        0        & 0        & 0       & \beta_m & \alpha_m
    \end{bmatrix} & =
    \overbrace{
        \begin{bmatrix}
            1         & 0         & 0      & \cdots    & 0 \\
            \lambda_2 & 1         & 0      & \cdots    & 0 \\
            0         & \lambda_3 & 1      & \cdots    & 0 \\
            \vdots    & \vdots    & \vdots & \ddots    & 0 \\
            0         & 0         & 0      & \lambda_m & 1
        \end{bmatrix}
    }^{L_m}
    \overbrace{
        \begin{bmatrix}
            \eta_1 & \beta_2 & 0       & \cdots & 0       \\
            0      & \eta_2  & \beta_3 & \cdots & 0       \\
            0      & 0       & \ddots  & \ddots & \vdots  \\
            \vdots & \vdots  & \vdots  & \ddots & \beta_m \\
            0      & 0       & 0       & 0      & \eta_m
        \end{bmatrix}}^{U_m}
\end{align*}

Now we rewrite the approximation using $L_m$ and $U_m$:
\[
    \mathbf{x}_m = \mathbf{x}_0 + \underbrace{V_m U_m^{-1}}_{P_m} \underbrace{L_m^{-1} \beta \mathbf{e}_1}_{\mathbf{z}_m},
    \quad \mathbf{z}_m =
    \begin{bmatrix}
        \zeta_1 \\
        \zeta_2 \\
        \vdots  \\
        \zeta_m
    \end{bmatrix},
    \quad P_m = [\mathbf{p}_1, \ldots, \mathbf{p}_m]
\]

\begin{align*}
    L_m \mathbf{z}_m                    & = \beta \mathbf{e}_1      \\
    \zeta_1                             & = \beta                   \\
    \lambda_2 \zeta_1 + \zeta_2         & = 0                       \\
    \vdots                                                          \\
    \lambda_{i+1} \zeta_i + \zeta_{i+1} & = 0, \quad i=1,\ldots,m-1
\end{align*}
\begin{align*}
    P_m U_m                                        & = V_m                              \\
    \eta_1 \mathbf{p}_1                            & = \mathbf{v}_1                     \\
    \beta_2 \mathbf{p}_1 + \eta_2 \mathbf{p}_2     & = \mathbf{v}_2                     \\
    \vdots                                                                              \\
    \beta_i \mathbf{p}_{i-1} + \eta_i \mathbf{p}_i & = \mathbf{v}_i, \quad i=2,\ldots,m \\
    \mathbf{p}_i = \frac{1}{\eta_i}(\mathbf{v}_i - \beta_i \mathbf{p}_{i-1})
\end{align*}

Then
\begin{align*}
    \mathbf{x}_m & = \mathbf{x}_0 + P_m \mathbf{z}_m                                                                                                \\
                 & = \mathbf{x}_0 + \sum_{i=1}^m \mathbf{p}_i \zeta_i = \mathbf{x}_0 + \sum_{i=1}^{m-1} \mathbf{p}_i \zeta_i + \mathbf{p}_m \zeta_m \\
                 & = \mathbf{x}_{m-1} + \zeta_m \mathbf{p}_m
\end{align*}

If we incorporate this into the Lanczos algorithm we get the \emph{conjugate gradient} (CG) method.

\subsection{Conjugate gradient (CG) method}
\begin{proposition}{}{}
    \begin{align*}
        \mathbf{r}_j & = \mathbf{b} - A \mathbf{x}_j, \quad j = 0, 1, \dots, m                                \\
        \mathbf{p}_j & = \frac{1}{\eta_j} (\mathbf{v}_j - \beta_j \mathbf{p}_{j-1}), \quad j = 1, 2, \dots, m
    \end{align*}
    Then:
    \begin{enumerate}[label=(\alph*)]
        \item $\langle \mathbf{r}_i, \mathbf{r}_j \rangle = 0$ for $i \neq j$ (residuals are orthogonal)
        \item $\langle \mathbf{p}_i, A \mathbf{p}_j \rangle = 0$ for $i \neq j$ ($A$-orthogonal search directions)
    \end{enumerate}
\end{proposition}

For a) The residual:
\begin{align*}
    \mathbf{r}_j & = \mathbf{b} - A \mathbf{x}_j                                                            \\
                 & = -\beta_{j+1} \mathbf{e}_j^\top \mathbf{y}_j \mathbf{v}_{j+1}, \quad j = 1, 2, \dots, m \\
                 & = \sigma \mathbf{v}_{j+1}, \quad \sigma = -\beta_{j+1} \mathbf{e}_j^\top \mathbf{y}_j
\end{align*}
Since $\mathbf{v}_j$ are orthogonal by construction, so are the residuals $\mathbf{r}_j$ for $j = 0, 1, \dots, m$.

For b) We have
\begin{align*}
    P_m                                                          & = \begin{bmatrix}
                                                                         \mathbf{p}_1 & \mathbf{p}_2 & \cdots & \mathbf{p}_m
                                                                     \end{bmatrix} \\
    P_m^\top A P_m                                               & = D \text{ (diagonal) }                               \\
    U_m^{-\top}\overbrace{V_m^\top A V_m}^{T_m=L_m U_m} U_m^{-1} & = D                                                   \\
    P_m^\top A P_m                                               & = U_m^{-\top} L_m U_m U_m^{-1} = U_m^{-\top} L_m = D
\end{align*}

Obviously, $P_m^\top A P_m$ is symmetric.
\begin{itemize}
    \item $U_m^{-\top}$ and $L_m$ are lower bidiagonal:
          \[
              U_m^{-\top} =
              \begin{bmatrix}
                  \frac{1}{\eta_1}               & 0                              & 0                & \cdots                             & 0                \\
                  -\frac{\beta_2}{\eta_1 \eta_2} & \frac{1}{\eta_2}               & 0                & \cdots                             & 0                \\
                  0                              & -\frac{\beta_3}{\eta_2 \eta_3} & \frac{1}{\eta_3} & \cdots                             & 0                \\
                  \vdots                         & \vdots                         & \vdots           & \ddots                             & 0                \\
                  0                              & 0                              & 0                & -\frac{\beta_m}{\eta_{m-1} \eta_m} & \frac{1}{\eta_m}
              \end{bmatrix},
              \quad
              L_m =
              \begin{bmatrix}
                  1         & 0         & 0      & \cdots    & 0 \\
                  \lambda_2 & 1         & 0      & \cdots    & 0 \\
                  0         & \lambda_3 & 1      & \cdots    & 0 \\
                  \vdots    & \vdots    & \vdots & \ddots    & 0 \\
                  0         & 0         & 0      & \lambda_m & 1
              \end{bmatrix}
          \]
    \item $U_m^{-\top} L_m$ is lower triangular:
          \[
              U_m^{-\top} L_m =
              \begin{bmatrix}
                  \frac{1}{\eta_1}               & 0                              & 0                & \cdots                             & 0                \\
                  -\frac{\beta_2}{\eta_1 \eta_2} & \frac{1}{\eta_2}               & 0                & \cdots                             & 0                \\
                  0                              & -\frac{\beta_3}{\eta_2 \eta_3} & \frac{1}{\eta_3} & \cdots                             & 0                \\
                  \vdots                         & \vdots                         & \vdots           & \ddots                             & 0                \\
                  0                              & 0                              & 0                & -\frac{\beta_m}{\eta_{m-1} \eta_m} & \frac{1}{\eta_m}
              \end{bmatrix}
          \]
    \item So: A lower triangular symmetric matrix is diagonal.
          \[
              P_m^\top A P_m = U_m^{-\top} L_m = D
          \]
\end{itemize}

\begin{align*}
    \mathbf{x}_m & = \mathbf{x}_0 + V_m \left(V_m^\top A V_m\right)^{-1} V_m^\top \mathbf{r}_0        \\
                 & = \mathbf{x}_0 + V_m T_m^{-1} \beta \mathbf{e}_1, \quad \beta = \|\mathbf{r}_0\|_2 \\
                 & = \mathbf{x}_0 + P_m \mathbf{z}_m = \mathbf{x}_{m-1} + \zeta_m \mathbf{p}_m        \\
    T_m          & = L_m U_m                                                                          \\
    P_m          & = V_m U_m^{-1}                                                                     \\
    \mathbf{z}_m & = L_m^{-1} \beta \mathbf{e}_1
\end{align*}

For each iteration $j$ with $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0 = \mathbf{p}_0$:
\begin{align*}
    \mathbf{x}_{j+1} & = \mathbf{x}_j + \alpha_j \mathbf{p}_j \Rightarrow \mathbf{r}_{j+1} = \mathbf{r}_j - \alpha_j A \mathbf{p}_j \\
    \mathbf{p}_{j+1} & = \mathbf{r}_{j+1} + \beta_j \mathbf{p}_j
\end{align*}
We know that:
\begin{align*}
    \langle \mathbf{r}_{j+1}, \mathbf{r}_j \rangle & = 0 \Rightarrow \alpha_j = \frac{\langle \mathbf{r}_j, \mathbf{r}_j \rangle}{\langle A \mathbf{p}_j, \mathbf{p}_j \rangle} = \frac{\|\mathbf{r}_j\|_2^2}{\langle \mathbf{p}_j, A \mathbf{p}_j \rangle} \\
    \langle \mathbf{r}_{j+1}, \mathbf{r}_j \rangle & = 0 \Rightarrow \beta_j = \frac{\langle \mathbf{r}_{j+1}, \mathbf{r}_{j+1} \rangle}{\langle \mathbf{r}_j, \mathbf{r}_j \rangle} = \frac{\|\mathbf{r}_{j+1}\|_2^2}{\|\mathbf{r}_j\|_2^2}
\end{align*}

Then the CG algorithm is:
\begin{algorithm}[H]
    \caption{Conjugate gradient (CG) method}
    \begin{algorithmic}[0]
        \Require $A, \mathbf{b}, \mathbf{x}_0, m$
        \State $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0$
        \State $\mathbf{p}_0 = \mathbf{r}_0$
        \For{$j = 0, 1, \ldots, m-1$}
        \State $\alpha_j = \dfrac{\|\mathbf{r}_j\|_2^2}{\langle \mathbf{p}_j, A \mathbf{p}_j \rangle}$
        \State $\mathbf{x}_{j+1} = \mathbf{x}_j + \alpha_j \mathbf{p}_j$
        \State $\mathbf{r}_{j+1} = \mathbf{r}_j - \alpha_j A \mathbf{p}_j$
        \State $\beta_{j+1} = \dfrac{\|\mathbf{r}_{j+1}\|_2^2}{\|\mathbf{r}_j\|_2^2}$
        \State $\mathbf{p}_{j+1} = \mathbf{r}_{j+1} + \beta_j \mathbf{p}_j$
        \If{$\|\mathbf{r}_{j+1}\|_2 < \text{tol}$} \textbf{Stop}
        \EndIf
        \EndFor
        \Return $\mathbf{x}_m$
    \end{algorithmic}
\end{algorithm}

\paragraph{Complexity.}
For every iteration $j$ we need to compute:
\begin{enumerate}
    \item One matrix-vector product $A\mathbf{p}_j$ (if $A$ is sparse, $\mathcal{O}(Nz(A))$) ($Nz(A)$ = number of nonzeros elements in $A$)
    \item 3 vector updates (axpy), $\mathcal{O}(n)$
    \item 2 inner products, $\mathcal{O}(n)$
\end{enumerate}
\textbf{Total:} $m\cdot \mathcal{O}(Nz(A) + n) = \mathcal{O}(m \cdot Nz(A) + m \cdot n)$ for $m$ iterations.
\paragraph{Memory.}
We need to store $(\mathbf{x}_j, \mathbf{r}_j, \mathbf{p}_j)$, i.e., $3n$ entries, and $A$ (if sparse, $\mathcal{O}(Nz(A))$).

\paragraph{Relation to Orthogonal polynomials.}
\begin{align*}
    \langle f, g \rangle & = \int_a^b w(x) f(x) g(x) \, dx , \quad w(x) > 0 \text{ (weight function)}    \\
    p_0(x)               & = 1                                                                           \\
    p_1(x)               & = x                                                                           \\
    p_n(x)               & = (x - a_n) p_{n-1}(x) - b_n p_{n-2}(x), \quad n \geq 2                       \\
    a_n                  & = \frac{\langle x p_{n-1}, p_{n-1} \rangle}{\langle p_{n-1}, p_{n-1} \rangle} \\
    b_n                  & = \frac{\langle x p_{n-2}, p_{n-2} \rangle}{\langle p_{n-2}, p_{n-2} \rangle}
\end{align*}
